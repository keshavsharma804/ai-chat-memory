# VectorLabs AI â€“ Chat with Memory + Web Search

An AI chat application with *long-term memory, **real-time web search, and a **ChatGPT-style UI* â€“ built as a full-stack project for learning and portfolio use.

- ğŸ§  *Vector memory* using Pinecone (stores past conversations per user)
- ğŸŒ *Real-time web search* using Tavily (latest news, facts, info)
- ğŸ¤ *LLM reasoning* using OpenRouter (Mistral-Nemo + embeddings)
- ğŸ–¥ *Modern React frontend* with multi-chat sessions and clean UX
- â˜ *Serverless backend* on Cloudflare Workers

---

## ğŸ”§ Tech Stack

*Backend*

- Cloudflare Workers
- Pinecone (Serverless index)
- Tavily Search API
- OpenRouter API
  - intfloat/e5-base-v2 (embeddings)
  - mistral-nemo (chat completions)
- JSON REST API (/chat, /health)

*Frontend*

- React + Vite
- Modern chat UI (sidebar, chat bubbles, loading state)
- Environment-based backend URL

---

## ğŸ§± Architecture

*High level flow:*

1. User sends a message from the React frontend â†’ /chat
2. Backend:
   - Creates an *embedding* for the message via OpenRouter
   - *Queries Pinecone* for similar past messages (user-scoped memory)
   - *Calls Tavily* to fetch fresh web results (news / facts)
   - Combines:
     - Memory (if relevant)
     - Web search results
     - LLMâ€™s own knowledge  
   - Generates a final reply using mistral-nemo on OpenRouter
   - Stores the new message back into Pinecone as a memory
3. Frontend displays reply, shows loading state, and maintains multi-chat sessions.

